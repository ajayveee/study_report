% v2-acmtog-sample.tex, dated March 7 2012
% This is a sample file for ACM Transactions on Graphics
%
% Compilation using 'acmtog.cls' - version 1.2 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.2 - March 2012
\documentclass{acmtog} % V1.2
\usepackage{amsmath}
%\acmVolume{VV}
%\acmNumber{N}
%\acmYear{YYYY}
%\acmMonth{Month}
%\acmArticleNum{XXX}
%\acmdoi{10.1145/XXXXXXX.YYYYYYY}

\acmVolume{28}
\acmNumber{4}
\acmYear{2015}
\acmMonth{September}
\acmArticleNum{106}
\doi{10.1145/1559755.1559763}

%ISSN
% \issn{1234-56789}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


\begin{document}

\markboth{Ajay Vijayakumaran Nair}{Survey of Ensemble Classifiers}

\title{Survey of Ensemble Classifiers} % title

\author{AJAY VIJAYAKUMARAN NAIR 
\affil{University of North Carolina at Charlotte}
}

\keywords{Classifier, ensemble, bagging, boosting, bayes optimal classifier, adaboost}

\maketitle

\begin{bottomstuff}
This report is the draft to a full masters study report in the making required as part of the Masters in Computer Science program at UNC Charlotte.
\end{bottomstuff}


\begin{abstract}
An interesting class of algorithms in the Machine Learning domain are the ensemble classification algorithms that try to improve the predictive power of weak classifiers through averaging or aggregation. The goal of this survey is to evaluate the building blocks of a typical ensemble classifier. An ensemble classifier, in most cases, performs far superior than its underlying individual base classifier. The survey begins by exploring some of the typical base classifiers that are used in an ensemble. Through the evaluation of ensemble techniques such as Bagging, Boosting and Bayes Optimal Classifier, the intuition and rationale of why an ensemble classifier outperforms a weak individual classifier is presented. An analysis of Adaboost learning classifier is also presented to review the mechanics of how an ensemble classification scheme achieves the boosted predictive power.
\end{abstract}

\section{Introduction}
A typical individual base classifier, in a supervised classification scheme, tries to reduce the in sample error there by expecting the in sample error to closely reflect the out of sample error. Trying to model the hypothesis purely based on the training data set to fit often results in a complex model that reduces the in sample error but not necessarily the out of sample error. The effectiveness of a classifier is usually measured in terms of the error which is a combination of primarily two factors - Bias and Variance. For real world problems, it is not practical to reduce both of the terms at the same time. A decrease in bias often leads to an increase in variance and vice-versa.

Ensemble classifiers try to solve this problem by training multiple classifiers that learn different patterns in the data and then combine the results to yield a conclusion that an otherwise individual classifier would not have been able to learn. The bagging technique does this by training multiple weak classifiers non-sequentially and later combining the predictions based on majority vote to make a decision in a binary classification problem. Boosting follows a sequential approach where a single weak classifier is trained sequentially in multiple boosting rounds to progressivley improve the model. A weak classifier is usually termed as a classifier that can predict an outcome better than chance. 


\section{Base Classifier}
A typical base classifer can be any of the well known classifiers such an k-NN, decision trees, perceptron or even SVMs. For this survey, we would restrict classifiers to be any of k-NN, decision trees or perceptron to focus on the ensemble techniques. 

A decision tree is a tree with nodes of the tree representing decisions based on a selected attribute of the data set. The branches are labelled with the outcome of the decision and represent the consequence of the decision made at the parent. A traversal till the leaf of the tree constitute a decision for a given data point and signifies the outcome. For decision tree ensembles, a decision stump, a one level decision tree, is often chosen as the weak learner. The decision stump consists of a single root node which constitutes a decision. 

For k-NN classification algorithms, k neighbors are chosen to decide the label of a test point. Given a test data point, the classification is based on how close the given point is to its neighbors. The number of neighbors is the value of k. For higher dimensional data vectors, a distance measure such an Euclidean or Manhattan distance is used. 


\section{Bagging}
Bootstrap Aggregation or Bagging is an ensemble technique which averages the prediction over a collection of bootstrap samples. A bootstrap sample is a sampling of the training data set to build weak models from. The sampling can be either with replacement so that the generated data samples for each of the models do not conform to a predefined bias, or it can be based on a model with a gaussian noise added so that the effect of bias in the sample selection is reduced.

With $n$ bootstrap samples, $n$ classifiers are trained to predict the outcome of a test data point $x$. The final outcome is decided based on averaging of all the outcomes of individual classifiers.
\begin{equation}
	\bar{f}(x) = \frac{1}{n}\sum^n_{i=1} f_i(x)
\label{eq:samplevar}
\end{equation}

Here, $\bar{f}(x)$ represents the bagged prediction, combining the predictions of the individual base classifiers.


\section{Boosting}
Boosting is also a technique which combines the output of many weak classifiers. It is similar only superficially to bagging in that boosting sequentially improves the predictive power of a weak classifier, whereas bagging involves combining outputs of many classifiers.
\section{Bayes Optimal Classifier}
\section{Adaboost}
\section{Conclusion}
\section{References}
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{acmtog-sample-bibfile}
                                % Sample .bib file with references that match those in
                                % the 'Specifications Document (V1.5)' as well containing
                                % 'legacy' bibs and bibs with 'alternate codings'.
                                % Gerry Murray - March 2012

 % \received{September 2008}{March 2009}

\end{document}
% End of v2-acmtog-sample.tex (March 2012) - Gerry Murray, ACM
